{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweets(Sentiment-analysis).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2PIVOaqTbsL"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHImCu2rTXsh",
        "outputId": "c58a0720-75f0-4306-a441-4e593d608585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIJepxJOTmhJ",
        "outputId": "7bab6a27-7e32-4ef4-ebce-0cde05bc29ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "!gdown 'https://drive.google.com/uc?id=1JffwGvZ-LAx_Cq4Bb4quyJAMsEf63vVz'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JffwGvZ-LAx_Cq4Bb4quyJAMsEf63vVz\n",
            "To: /content/trainingandtestdata.zip\n",
            "81.4MB [00:01, 55.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjfQgOQOTvN_",
        "outputId": "3fbf50e2-6927-4e93-8d0e-11094249f2b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!unzip trainingandtestdata.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  trainingandtestdata.zip\n",
            "  inflating: testdata.manual.2009.06.14.csv  \n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBmn3hwtT02V"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omapUveZT1ue"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "# tensorflow_datasets contains tokenizer function"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USPEKs9ykuTL"
      },
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "\n",
        "train_data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", header=None, names=cols, engine=\"python\", encoding=\"latin1\")\n",
        "test_data = pd.read_csv(\"testdata.manual.2009.06.14.csv\", header=None, names=cols, engine=\"python\", encoding=\"latin1\")\n",
        "\n",
        "data = train_data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQVW03ComUMd",
        "outputId": "0b58cb44-a63a-4d94-8802-99d20a2db1be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "print(len(data))\n",
        "data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1600000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                               text\n",
              "0          0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  ...  is upset that he can't update his Facebook by ...\n",
              "2          0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3          0  ...    my whole body feels itchy and like its on fire \n",
              "4          0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A44R4FpqnbxA",
        "outputId": "b447476d-f99c-4661-d2be-a94b4c2c920a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data.drop([\"id\", \"date\", \"query\", \"user\"], axis=1, inplace=True)\n",
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                               text\n",
              "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  is upset that he can't update his Facebook by ...\n",
              "2          0  @Kenichan I dived many times for the ball. Man...\n",
              "3          0    my whole body feels itchy and like its on fire \n",
              "4          0  @nationwideclass no, it's not behaving at all...."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG9k_sGToLJT"
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "  tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "  tweet = re.sub(r\"@[A-Za-z0-9]+\", \" \", tweet)\n",
        "  # get rid of all mentions in the tweet\n",
        "  tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", \" \", tweet)\n",
        "  # get rid of all urls\n",
        "  tweet = re.sub(r\"[^a-zA-Z.!?']\", \" \", tweet)\n",
        "  # to only keep letters and standard punctuation\n",
        "  tweet = re.sub(r\" +\", \" \", tweet)\n",
        "  # replace multiple whitespaces by a single whitespace\n",
        "  return tweet"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csM3OmiGqVs0",
        "outputId": "2dc2ec1b-423e-4cb0-ea5f-6cc4c054bce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data.text]\n",
        "print(data_clean[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Awww that's a bummer. You shoulda got David Carr of Third Day to do it. D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKdqSQwzqx0i"
      },
      "source": [
        "data_labels = data.iloc[0:, 0:1]\n",
        "data_labels = np.array(data_labels)\n",
        "data_labels[data_labels == 4] = 1\n",
        "# data_labels[data_labels == 2] = 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGAs5asNrYBM",
        "outputId": "fcae7a2b-e231-4791-ed0d-d238afd91c33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(np.unique(data_labels))\n",
        "print(len(data_labels))\n",
        "print(len(data_clean))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1]\n",
            "1600000\n",
            "1600000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czFkebTItuBq"
      },
      "source": [
        "# converting words into numbers using tensorflow tokenizer\n",
        "\n",
        "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    data_clean, target_vocab_size=2**16\n",
        ")\n",
        "\n",
        "data_inputs = [tokenizer.encode(sentence) for sentence in data_clean]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHm3FXY20g3E",
        "outputId": "dae53230-73d5-4cfe-93e2-6657f3503d49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(len(data_inputs[0]))\n",
        "print(data_inputs[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21\n",
            "[65316, 1570, 113, 65323, 10, 6, 3553, 1, 135, 5262, 50, 1484, 38165, 16, 13337, 606, 2, 49, 33, 1, 65352]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSXR0TB8zNBs"
      },
      "source": [
        "# Add padding to make length of all sentences equal\n",
        "\n",
        "MAX_LEN = max([len(sentence) for sentence in data_inputs])\n",
        "\n",
        "# this makes all our sentences of length=MAX_LEN by adding 0s at the end\n",
        "data_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs,\n",
        "                                                            value=0,\n",
        "                                                            padding=\"post\",\n",
        "                                                            maxlen=MAX_LEN)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb08thME0j7C",
        "outputId": "41127659-ffc3-4402-a2f9-17ab56ec64a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "print(len(data_inputs[0]))\n",
        "print(MAX_LEN)\n",
        "print(data_inputs[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73\n",
            "73\n",
            "[65316  1570   113 65323    10     6  3553     1   135  5262    50  1484\n",
            " 38165    16 13337   606     2    49    33     1 65352     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK_8GiLi0d4S",
        "outputId": "7dddc4c0-17ab-45c2-f95c-0a9cf011e032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# splitting dataset into train and test set\n",
        "\n",
        "test_idx = np.random.randint(0, 800000, 8000)\n",
        "# 8000 random negative tweets stored in test_idx right now\n",
        "\n",
        "test_idx = np.concatenate((test_idx, test_idx+800000))\n",
        "# this combines 8000 random negative tweets and 8000 random positive tweets to give us total 16000 testing tweets\n",
        "\n",
        "test_inputs = data_inputs[test_idx]\n",
        "test_labels = data_labels[test_idx]\n",
        "\n",
        "train_inputs = np.delete(data_inputs, test_idx, axis=0)\n",
        "train_labels = np.delete(data_labels, test_idx, axis=0)\n",
        "\n",
        "print(len(test_inputs))\n",
        "print(len(test_labels))\n",
        "print(len(train_inputs))\n",
        "print(len(train_labels))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16000\n",
            "16000\n",
            "1584082\n",
            "1584082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_aTeBjV6yAL"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYZ3X9rb6yxh"
      },
      "source": [
        "class DCNN(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               emb_dim=128,\n",
        "               nb_filters=50,\n",
        "               FFN_units=512,\n",
        "               nb_classes=2,\n",
        "               dropout_rate=0.1,\n",
        "               training=False,\n",
        "               name=\"dcnn\"):\n",
        "    super(DCNN, self).__init__(name=name)\n",
        "\n",
        "    # embedding layer\n",
        "    self.embedding = layers.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "    # this create feature maps to find similarity between 2 simultaneous words(dimension of filter map=2xemb_dim)\n",
        "    # we use 1d convolutions since width of feature maps is same as width of input matrix\n",
        "    self.bigram = layers.Conv1D(filters=nb_filters, kernel_size=2, padding=\"valid\", activation=\"relu\")\n",
        "    self.pool_1 = layers.GlobalMaxPool1D()\n",
        "    # this create feature maps to find similarity between 3 simultaneous words(dimension of filter map=3xemb_dim)\n",
        "    self.trigram = layers.Conv1D(filters=nb_filters, kernel_size=3, padding=\"valid\", activation=\"relu\")\n",
        "    self.pool_2 = layers.GlobalMaxPool1D()\n",
        "    # this create feature maps to find similarity between 4 simultaneous words(dimension of filter map=4xemb_dim)\n",
        "    self.fourgram = layers.Conv1D(filters=nb_filters, kernel_size=4, padding=\"valid\", activation=\"relu\")\n",
        "    self.pool_3 = layers.GlobalMaxPool1D()\n",
        "\n",
        "    # full connection layers\n",
        "    self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
        "    self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "    # output layer\n",
        "    if nb_classes == 2:\n",
        "      self.last_dense = layers.Dense(units=1, activation=\"sigmoid\")\n",
        "    else:\n",
        "      self.last_dense = layers.Dense(units=nb_units, activation=\"softmax\")\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    x = self.embedding(inputs)\n",
        "\n",
        "    x_1 = self.bigram(x)\n",
        "    x_1 = self.pool_1(x_1)\n",
        "    x_2 = self.trigram(x)\n",
        "    x_2 = self.pool_2(x_2)\n",
        "    x_3 = self.fourgram(x)\n",
        "    x_3 = self.pool_3(x_3)\n",
        "\n",
        "    merged = tf.concat([x_1, x_2, x_3], axis=-1)\n",
        "    # size = (batch_size, 3*nb_filters)\n",
        "    merged = self.dense_1(merged)\n",
        "    merged = self.dropout(merged, training)\n",
        "    output = self.last_dense(merged)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xGH7qNfCFRA"
      },
      "source": [
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "EMB_DIM = 200\n",
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = len(np.unique(data_labels))\n",
        "DROPOUT_RATE = 0.2\n",
        "BATCH_SIZE = 32\n",
        "NB_EPOCHS = 2"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFqFzipXC14e"
      },
      "source": [
        "dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
        "            emb_dim=EMB_DIM,\n",
        "            nb_filters=NB_FILTERS,\n",
        "            FFN_units=FFN_UNITS,\n",
        "            nb_classes=NB_CLASSES,\n",
        "            dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TwFlfP_DjCD"
      },
      "source": [
        "if NB_CLASSES == 2:\n",
        "  dcnn.compile(loss=\"binary_crossentropy\",\n",
        "               optimizer=\"adam\",\n",
        "               metrics=[\"accuracy\"])\n",
        "else:\n",
        "  dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "               optimizer=\"adam\",\n",
        "               metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRAfFcIUEs-U"
      },
      "source": [
        "checkpoint_path = \"drive/My Drive/models/tweets-sentiment/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(dcnn=dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print(\"Latest checkpoint restored!\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-KWSMO9F6AX",
        "outputId": "ee7aa436-d169-4f19-fabc-920f0ccd7d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "dcnn.fit(train_inputs,\n",
        "         train_labels,\n",
        "         batch_size=BATCH_SIZE,\n",
        "         epochs=NB_EPOCHS)\n",
        "\n",
        "ckpt_manager.save()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "49503/49503 [==============================] - 6340s 128ms/step - loss: 0.3989 - accuracy: 0.8197\n",
            "Epoch 2/2\n",
            " 7652/49503 [===>..........................] - ETA: 1:28:58 - loss: 0.3269 - accuracy: 0.8592Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2tCyxXOIMKv"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8nuI1r2INEJ",
        "outputId": "76582979-7d1b-471e-8c6b-157d8b407983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "results = dcnn.evaluate(test_inputs, \n",
        "                        test_labels, \n",
        "                        batch_size=BATCH_SIZE)\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 2s 4ms/step - loss: 0.3684 - accuracy: 0.8409\n",
            "[0.3684089481830597, 0.8408750295639038]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-FIIB2BImUG",
        "outputId": "489134ba-1817-4b20-c398-7d7b54bab9e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dcnn(np.array([tokenizer.encode('I love you')]), training=False).numpy()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.44495186]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-5kNViAI8YN",
        "outputId": "f471fc21-3cf2-4185-aa87-ea203f963b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dcnn(np.array([tokenizer.encode('I hate you')]), training=False).numpy()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.44495186]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jBS1wMEI_IF",
        "outputId": "5f7c5ed3-4f1d-46c5-b575-5faaa91a0d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dcnn(np.array([tokenizer.encode('I wish no one ever have to do that again')]), training=False).numpy()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0143242]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piNpdJOXJFlL",
        "outputId": "7509d2d5-e244-4b78-bdd5-0dd9a79348c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dcnn(np.array([tokenizer.encode('You are so funny!')]), training=False).numpy()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.94882584]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}