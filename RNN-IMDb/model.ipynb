{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "In this example we will use IMDb dataset to train an recurrent neural network how to \"read\" movie reviews and guess whether the author liked the movie or not from them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)\n",
    "# load data with only 20000 of the most popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n1\n"
    }
   ],
   "source": [
    "print(x_train[0])  # each word is encoded as a number\n",
    "print(y_train[0])  # 1 means liked 0 means not liked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=80)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=80)\n",
    "# this means we are only going to look at the first 80 words in each review\n",
    "# truncating backprop through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\nWARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, None, 128)         2560000   \n_________________________________________________________________\nlstm (LSTM)                  (None, None, 128)         131584    \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 128)               131584    \n_________________________________________________________________\ndense (Dense)                (None, 1)                 129       \n=================================================================\nTotal params: 2,823,297\nTrainable params: 2,823,297\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128))\n",
    "# Embedding layer - this is just a step that converts the input data into dense vectors of fixed\n",
    "# size that's better suited for a neural network. You generally see this in conjunction with index-based text data like\n",
    "# we have here. The 20,000 indicates the vocabulary size (remember we only extracted the top 20,000 words) and 128\n",
    "# is the output dimension.\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/5\n782/782 - 709s - loss: 0.4393 - accuracy: 0.7946 - val_loss: 0.3849 - val_accuracy: 0.8379\nEpoch 2/5\n782/782 - 741s - loss: 0.2624 - accuracy: 0.8942 - val_loss: 0.4239 - val_accuracy: 0.8250\nEpoch 3/5\n782/782 - 738s - loss: 0.1715 - accuracy: 0.9368 - val_loss: 0.4346 - val_accuracy: 0.8237\nEpoch 4/5\n782/782 - 738s - loss: 0.1173 - accuracy: 0.9586 - val_loss: 0.5433 - val_accuracy: 0.8180\nEpoch 5/5\n782/782 - 752s - loss: 0.0810 - accuracy: 0.9725 - val_loss: 0.5695 - val_accuracy: 0.8009\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1ba102a3648>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "782/782 - 26s - loss: 0.5695 - accuracy: 0.8009\nTest score: 0.5694915056228638\nTest accuracy: 0.8009200096130371\n"
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=32,\n",
    "                            verbose=2)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Prediction: Liked\nReal: Liked\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.reshape(x_train[0], (1, -1))\n",
    "pred = model.predict(data)\n",
    "\n",
    "if pred>=0.5:\n",
    "    print(\"Prediction: Liked\")\n",
    "else:\n",
    "    print(\"Prediction: Not Liked\")\n",
    "\n",
    "if y_train[0]==1:\n",
    "    print(\"Real: Liked\")\n",
    "else:\n",
    "    print(\"Real: Not Liked\")"
   ]
  }
 ]
}